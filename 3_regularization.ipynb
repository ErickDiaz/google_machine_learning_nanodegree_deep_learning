{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (51909, 28, 28) (51909,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (51909, 784) (51909, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#logistic regression with regularization\n",
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #dada\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size));\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels));\n",
    "    regularization_factor = tf.placeholder(tf.float32);\n",
    "    tf_valid_dataset = tf.constant(valid_dataset);\n",
    "    tf_test_dataset = tf.constant(test_dataset);\n",
    "    \n",
    "    #Variables(weights and biases)\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size*image_size,num_labels]));\n",
    "    biases = tf.Variable(tf.zeros(num_labels));\n",
    "    \n",
    "    #training computations\n",
    "    regularization = tf.nn.l2_loss(weights);\n",
    "    logits = tf.matmul(tf_train_dataset,weights) + biases;\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,tf_train_labels))+ (regularization_factor*regularization);\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss);\n",
    "    \n",
    "    #predictions\n",
    "    train_prediction = tf.nn.softmax(logits);\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset,weights)+biases);\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset,weights)+biases);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized for reg:0.0001\n",
      "Validation accuracy: 79.0%\n",
      "Test accuracy: 86.4%\n",
      "Initialized for reg:0.000125892541179\n",
      "Validation accuracy: 79.5%\n",
      "Test accuracy: 86.8%\n",
      "Initialized for reg:0.000158489319246\n",
      "Validation accuracy: 80.2%\n",
      "Test accuracy: 87.0%\n",
      "Initialized for reg:0.000199526231497\n",
      "Validation accuracy: 80.0%\n",
      "Test accuracy: 87.1%\n",
      "Initialized for reg:0.000251188643151\n",
      "Validation accuracy: 80.1%\n",
      "Test accuracy: 87.0%\n",
      "Initialized for reg:0.000316227766017\n",
      "Validation accuracy: 80.5%\n",
      "Test accuracy: 87.6%\n",
      "Initialized for reg:0.000398107170553\n",
      "Validation accuracy: 81.0%\n",
      "Test accuracy: 87.9%\n",
      "Initialized for reg:0.000501187233627\n",
      "Validation accuracy: 81.4%\n",
      "Test accuracy: 88.4%\n",
      "Initialized for reg:0.00063095734448\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.3%\n",
      "Initialized for reg:0.000794328234724\n",
      "Validation accuracy: 82.0%\n",
      "Test accuracy: 88.6%\n",
      "Initialized for reg:0.001\n",
      "Validation accuracy: 82.1%\n",
      "Test accuracy: 88.8%\n",
      "Initialized for reg:0.00125892541179\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 89.0%\n",
      "Initialized for reg:0.00158489319246\n",
      "Validation accuracy: 82.5%\n",
      "Test accuracy: 89.0%\n",
      "Initialized for reg:0.00199526231497\n",
      "Validation accuracy: 82.6%\n",
      "Test accuracy: 89.0%\n",
      "Initialized for reg:0.00251188643151\n",
      "Validation accuracy: 82.7%\n",
      "Test accuracy: 89.1%\n",
      "Initialized for reg:0.00316227766017\n",
      "Validation accuracy: 82.8%\n",
      "Test accuracy: 89.0%\n",
      "Initialized for reg:0.00398107170553\n",
      "Validation accuracy: 82.7%\n",
      "Test accuracy: 88.9%\n",
      "Initialized for reg:0.00501187233627\n",
      "Validation accuracy: 82.4%\n",
      "Test accuracy: 88.9%\n",
      "Initialized for reg:0.0063095734448\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 88.7%\n",
      "Initialized for reg:0.00794328234724\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 88.6%\n",
      "Initialized for reg:0.01\n",
      "Validation accuracy: 82.1%\n",
      "Test accuracy: 88.5%\n",
      "Initialized for reg:0.0125892541179\n",
      "Validation accuracy: 82.0%\n",
      "Test accuracy: 88.3%\n",
      "Initialized for reg:0.0158489319246\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 88.3%\n",
      "Initialized for reg:0.0199526231497\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.0%\n",
      "Initialized for reg:0.0251188643151\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.0%\n",
      "Initialized for reg:0.0316227766017\n",
      "Validation accuracy: 81.1%\n",
      "Test accuracy: 87.8%\n",
      "Initialized for reg:0.0398107170553\n",
      "Validation accuracy: 80.8%\n",
      "Test accuracy: 87.5%\n",
      "Initialized for reg:0.0501187233627\n",
      "Validation accuracy: 80.3%\n",
      "Test accuracy: 87.2%\n",
      "Initialized for reg:0.063095734448\n",
      "Validation accuracy: 80.0%\n",
      "Test accuracy: 86.5%\n",
      "Initialized for reg:0.0794328234724\n",
      "Validation accuracy: 79.0%\n",
      "Test accuracy: 85.6%\n",
      "Initialized for reg:0.1\n",
      "Validation accuracy: 77.9%\n",
      "Test accuracy: 84.3%\n",
      "Initialized for reg:0.125892541179\n",
      "Validation accuracy: 76.6%\n",
      "Test accuracy: 82.6%\n",
      "Initialized for reg:0.158489319246\n",
      "Validation accuracy: 75.0%\n",
      "Test accuracy: 81.0%\n",
      "Initialized for reg:0.199526231497\n",
      "Validation accuracy: 73.2%\n",
      "Test accuracy: 79.4%\n",
      "Initialized for reg:0.251188643151\n",
      "Validation accuracy: 71.3%\n",
      "Test accuracy: 77.5%\n",
      "Initialized for reg:0.316227766017\n",
      "Validation accuracy: 68.7%\n",
      "Test accuracy: 74.6%\n",
      "Initialized for reg:0.398107170554\n",
      "Validation accuracy: 65.4%\n",
      "Test accuracy: 70.8%\n",
      "Initialized for reg:0.501187233627\n",
      "Validation accuracy: 61.0%\n",
      "Test accuracy: 66.2%\n",
      "Initialized for reg:0.63095734448\n",
      "Validation accuracy: 57.1%\n",
      "Test accuracy: 61.6%\n"
     ]
    }
   ],
   "source": [
    "#run and evaluate\n",
    "num_steps = 3001\n",
    "regularizators = [pow(10,i) for i in np.arange(-4, -0.1, 0.1)];\n",
    "train_accuracy = [];\n",
    "valid_accuracy = [];\n",
    "test_accuracy = [];\n",
    "\n",
    "for regul_factor in regularizators:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run();\n",
    "        print(\"Initialized for reg:\"+str(regul_factor));\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size);\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset:batch_data,tf_train_labels:batch_labels , regularization_factor : regul_factor };\n",
    "            _,l,predictions =  session.run([optimizer,loss,train_prediction],feed_dict= feed_dict);\n",
    "\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels));\n",
    "        valid_accuracy.append(accuracy(\n",
    "                valid_prediction.eval(), valid_labels));\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels));\n",
    "        test_accuracy.append(accuracy(test_prediction.eval(), test_labels));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEOCAYAAABy7Vf3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndxGRMqTiLiJShlTcRUTK0P8DAXbqoQRtoQ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4ba49e1090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.semilogx(regularizators, valid_accuracy)\n",
    "plot.grid(True)\n",
    "plot.title('Regularization factor evaluation in CV dataset')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEOCAYAAABy7Vf3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4ba49e1550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.semilogx(regularizators, test_accuracy)\n",
    "plot.grid(True)\n",
    "plot.title('Regularization factor evaluation in test dataset')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization factor selected for logistic classifier: 0.00158489319246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Neural newtork(relu hidden units) with regularization\n",
    "batch_size = 128;\n",
    "graph = tf.Graph();\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size));\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels));\n",
    "    tf_validation_dataset = tf.constant(valid_dataset);\n",
    "    tf_test_set = tf.constant(test_dataset);\n",
    "    regularization_factor = tf.placeholder(tf.float32);\n",
    "    \n",
    "    #variables\n",
    "    weights_layer1 = tf.Variable(tf.truncated_normal([image_size*image_size,1024]));\n",
    "    biases_layer1 = tf.Variable(tf.zeros([1024]));\n",
    "    \n",
    "    #computaton of layer1\n",
    "    logits_layer1 = tf.matmul(tf_train_dataset,weights_layer1)+biases_layer1;\n",
    "    relus_output = tf.nn.relu(logits_layer1);\n",
    "    \n",
    "    #parametes layer2\n",
    "    weights_layer2 = tf.Variable(tf.truncated_normal([1024,10]));\n",
    "    biases_layer2 = tf.Variable(tf.zeros([10]));\n",
    "    \n",
    "    #computation layer2\n",
    "    regularization = tf.nn.l2_loss(weights_layer1) + tf.nn.l2_loss(weights_layer2);\n",
    "    logits_layer2  = tf.matmul(relus_output,weights_layer2)+biases_layer2;\n",
    "    loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits_layer2,tf_train_labels)) + (regularization_factor*regularization) ;\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss);\n",
    "    \n",
    "    #predictions\n",
    "    train_prediction = tf.nn.softmax(logits_layer2);\n",
    "    validation_relus_output = tf.nn.relu(tf.matmul(tf_validation_dataset,weights_layer1)+biases_layer1);\n",
    "    validation_prediction  = tf.nn.softmax(tf.matmul(validation_relus_output,weights_layer2)+biases_layer2);\n",
    "    test_relus_output = tf.nn.relu(tf.matmul(tf_test_set,weights_layer1)+biases_layer1);\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_relus_output,weights_layer2)+biases_layer2);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized variables for reg factor:0.0001\n",
      "Validation accuracy: 82.6%\n",
      "Test accuracy: 89.7%\n",
      "Initialized variables for reg factor:0.000125892541179\n",
      "Validation accuracy: 83.1%\n",
      "Test accuracy: 90.1%\n",
      "Initialized variables for reg factor:0.000158489319246\n",
      "Validation accuracy: 82.7%\n",
      "Test accuracy: 89.3%\n",
      "Initialized variables for reg factor:0.000199526231497\n",
      "Validation accuracy: 83.0%\n",
      "Test accuracy: 90.1%\n",
      "Initialized variables for reg factor:0.000251188643151\n",
      "Validation accuracy: 83.3%\n",
      "Test accuracy: 90.1%\n",
      "Initialized variables for reg factor:0.000316227766017\n",
      "Validation accuracy: 83.5%\n",
      "Test accuracy: 90.4%\n",
      "Initialized variables for reg factor:0.000398107170553\n",
      "Validation accuracy: 83.8%\n",
      "Test accuracy: 90.9%\n",
      "Initialized variables for reg factor:0.000501187233627\n",
      "Validation accuracy: 84.0%\n",
      "Test accuracy: 90.4%\n",
      "Initialized variables for reg factor:0.00063095734448\n",
      "Validation accuracy: 84.5%\n",
      "Test accuracy: 91.2%\n",
      "Initialized variables for reg factor:0.000794328234724\n",
      "Validation accuracy: 85.5%\n",
      "Test accuracy: 91.7%\n",
      "Initialized variables for reg factor:0.001\n",
      "Validation accuracy: 86.2%\n",
      "Test accuracy: 92.3%\n",
      "Initialized variables for reg factor:0.00125892541179\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regularizators = [pow(10,i) for i in np.arange(-4,-0.1,0.1)]\n",
    "train_accuracy = [];\n",
    "valid_accuracy = [];\n",
    "test_accuracy = [];\n",
    "\n",
    "for regul_factor in regularizators:\n",
    "    with tf.Session(graph = graph) as session:\n",
    "        tf.initialize_all_variables().run();\n",
    "        print(\"Initialized variables for reg factor:\"+str(regul_factor));\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size);\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :];\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :];\n",
    "            feed_dict = {tf_train_dataset:batch_data,tf_train_labels:batch_labels,regularization_factor:regul_factor};\n",
    "            _,l,predictions = session.run([optimizer,loss,train_prediction],feed_dict = feed_dict);\n",
    "        \n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                validation_prediction.eval(), valid_labels));\n",
    "        valid_accuracy.append(accuracy(\n",
    "                validation_prediction.eval(), valid_labels));\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels));\n",
    "        test_accuracy.append(accuracy(test_prediction.eval(), test_labels));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot.semilogx(regularizators, valid_accuracy)\n",
    "plot.grid(True)\n",
    "plot.title('Regularization factor evaluation in CV dataset')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot.semilogx(regularizators, test_accuracy)\n",
    "plot.grid(True)\n",
    "plot.title('Regularization factor evaluation in test dataset')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization factor chosed for neural network: 0.00158489319246"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#i will use 20 batches with  a smaller training set\n",
    "\n",
    "num_steps = 20;\n",
    "reduced_dataset = train_dataset[:512, :]\n",
    "reduced_labels = train_labels[:512]\n",
    "with tf.Session(graph= graph) as session:\n",
    "    tf.initialize_all_variables().run();\n",
    "    print(\"Variables initialized\");\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (reduced_labels.shape[0] - batch_size);\n",
    "        batch_data = reduced_dataset[offset:(offset + batch_size), :];\n",
    "        batch_labels = reduced_labels[offset:(offset + batch_size), :];\n",
    "        feed_dict = {tf_train_dataset:batch_data,tf_train_labels:batch_labels,regularization_factor:0};\n",
    "        _,l,predictions = session.run([optimizer,loss,train_prediction],feed_dict = feed_dict);\n",
    "        \n",
    "        print('Loss at step %d: %f' % (step, l))\n",
    "        print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                validation_prediction.eval(), valid_labels));\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude theres overfitting watching that training accuracy is 100% but validation is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128;\n",
    "graph = tf.Graph();\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size));\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels));\n",
    "    tf_validation_dataset = tf.constant(valid_dataset);\n",
    "    tf_test_set = tf.constant(test_dataset);\n",
    "    regularization_factor = tf.placeholder(tf.float32);\n",
    "    weights_layer1=tf.Variable(tf.truncated_normal([image_size*image_size,1024]));\n",
    "    biases_layer1 = tf.Variable(tf_zeros([1024]));\n",
    "    \n",
    "    logits_layer1 = tf.matmul(tf_train_dataset,weights_layer1)+biases_layer1;\n",
    "    relus_output = tf.nn.relu(logits);\n",
    "    \n",
    "    droput_layer = tf.nn.droput(relus_output,0.5);\n",
    "    \n",
    "    weights_layer2 = tf.Variable(tf.truncated_normal([1024,10]));\n",
    "    biases_layer2 = tf.Variable(zeros([10]));\n",
    "    \n",
    "    regularization = tf.nn.l2_loss(weights_layer1) + tf.nn.l2_loss(weights_layer2);\n",
    "    logits_layer2 = tf.matmul(relus_output,weights_layer2)+biases_layer2;\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits_layer2,tf_train_labels))+(regularization*regularization_factor);\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss);\n",
    "    train_prediction = tf.nn.sofmax(logits_layer2);\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
