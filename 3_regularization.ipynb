{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (51909, 28, 28) (51909,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (51909, 784) (51909, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#logistic regression with regularization\n",
    "batch_size = 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #dada\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size));\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels));\n",
    "    regularization_factor = tf.placeholder(tf.float32);\n",
    "    tf_valid_dataset = tf.constant(valid_dataset);\n",
    "    tf_test_dataset = tf.constant(test_dataset);\n",
    "    \n",
    "    #Variables(weights and biases)\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size*image_size,num_labels]));\n",
    "    biases = tf.Variable(tf.zeros(num_labels));\n",
    "    \n",
    "    #training computations\n",
    "    regularization = tf.nn.l2_loss(weights);\n",
    "    logits = tf.matmul(tf_train_dataset,weights) + biases;\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,tf_train_labels))+ (regularization_factor*regularization);\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss);\n",
    "    \n",
    "    #predictions\n",
    "    train_prediction = tf.nn.softmax(logits);\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset,weights)+biases);\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset,weights)+biases);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized for reg:0.0001\n",
      "Validation accuracy: 80.0%\n",
      "Test accuracy: 86.7%\n",
      "Initialized for reg:0.000125892541179\n",
      "Validation accuracy: 79.3%\n",
      "Test accuracy: 86.7%\n",
      "Initialized for reg:0.000158489319246\n",
      "Validation accuracy: 80.2%\n",
      "Test accuracy: 86.9%\n",
      "Initialized for reg:0.000199526231497\n",
      "Validation accuracy: 80.2%\n",
      "Test accuracy: 87.3%\n",
      "Initialized for reg:0.000251188643151\n",
      "Validation accuracy: 80.4%\n",
      "Test accuracy: 87.2%\n",
      "Initialized for reg:0.000316227766017\n",
      "Validation accuracy: 80.5%\n",
      "Test accuracy: 87.6%\n",
      "Initialized for reg:0.000398107170553\n",
      "Validation accuracy: 81.1%\n",
      "Test accuracy: 88.1%\n",
      "Initialized for reg:0.000501187233627\n",
      "Validation accuracy: 81.7%\n",
      "Test accuracy: 88.3%\n",
      "Initialized for reg:0.00063095734448\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 88.3%\n",
      "Initialized for reg:0.000794328234724\n",
      "Validation accuracy: 82.1%\n",
      "Test accuracy: 88.6%\n",
      "Initialized for reg:0.001\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 89.0%\n",
      "Initialized for reg:0.00125892541179\n",
      "Validation accuracy: 82.6%\n",
      "Test accuracy: 89.1%\n",
      "Initialized for reg:0.00158489319246\n",
      "Validation accuracy: 82.6%\n",
      "Test accuracy: 89.1%\n",
      "Initialized for reg:0.00199526231497\n",
      "Validation accuracy: 82.6%\n",
      "Test accuracy: 89.1%\n",
      "Initialized for reg:0.00251188643151\n",
      "Validation accuracy: 82.7%\n",
      "Test accuracy: 89.1%\n",
      "Initialized for reg:0.00316227766017\n",
      "Validation accuracy: 82.7%\n",
      "Test accuracy: 89.1%\n",
      "Initialized for reg:0.00398107170553\n",
      "Validation accuracy: 82.7%\n",
      "Test accuracy: 88.9%\n",
      "Initialized for reg:0.00501187233627\n",
      "Validation accuracy: 82.4%\n",
      "Test accuracy: 88.9%\n",
      "Initialized for reg:0.0063095734448\n",
      "Validation accuracy: 82.3%\n",
      "Test accuracy: 88.7%\n",
      "Initialized for reg:0.00794328234724\n",
      "Validation accuracy: 82.2%\n",
      "Test accuracy: 88.6%\n",
      "Initialized for reg:0.01\n",
      "Validation accuracy: 82.1%\n",
      "Test accuracy: 88.5%\n",
      "Initialized for reg:0.0125892541179\n",
      "Validation accuracy: 82.0%\n",
      "Test accuracy: 88.3%\n",
      "Initialized for reg:0.0158489319246\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 88.3%\n",
      "Initialized for reg:0.0199526231497\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.0%\n",
      "Initialized for reg:0.0251188643151\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.0%\n",
      "Initialized for reg:0.0316227766017\n",
      "Validation accuracy: 81.1%\n",
      "Test accuracy: 87.8%\n",
      "Initialized for reg:0.0398107170553\n",
      "Validation accuracy: 80.8%\n",
      "Test accuracy: 87.5%\n",
      "Initialized for reg:0.0501187233627\n",
      "Validation accuracy: 80.3%\n",
      "Test accuracy: 87.2%\n",
      "Initialized for reg:0.063095734448\n",
      "Validation accuracy: 80.0%\n",
      "Test accuracy: 86.5%\n",
      "Initialized for reg:0.0794328234724\n",
      "Validation accuracy: 79.0%\n",
      "Test accuracy: 85.6%\n",
      "Initialized for reg:0.1\n",
      "Validation accuracy: 77.9%\n",
      "Test accuracy: 84.3%\n",
      "Initialized for reg:0.125892541179\n",
      "Validation accuracy: 76.6%\n",
      "Test accuracy: 82.6%\n",
      "Initialized for reg:0.158489319246\n",
      "Validation accuracy: 75.0%\n",
      "Test accuracy: 81.0%\n",
      "Initialized for reg:0.199526231497\n",
      "Validation accuracy: 73.2%\n",
      "Test accuracy: 79.4%\n",
      "Initialized for reg:0.251188643151\n",
      "Validation accuracy: 71.3%\n",
      "Test accuracy: 77.5%\n",
      "Initialized for reg:0.316227766017\n",
      "Validation accuracy: 68.7%\n",
      "Test accuracy: 74.6%\n",
      "Initialized for reg:0.398107170554\n",
      "Validation accuracy: 65.4%\n",
      "Test accuracy: 70.8%\n",
      "Initialized for reg:0.501187233627\n",
      "Validation accuracy: 61.0%\n",
      "Test accuracy: 66.2%\n",
      "Initialized for reg:0.63095734448\n",
      "Validation accuracy: 57.1%\n",
      "Test accuracy: 61.6%\n"
     ]
    }
   ],
   "source": [
    "#run and evaluate\n",
    "num_steps = 3001\n",
    "regularizators = [pow(10,i) for i in np.arange(-4, -0.1, 0.1)];\n",
    "train_accuracy = [];\n",
    "valid_accuracy = [];\n",
    "test_accuracy = [];\n",
    "\n",
    "for regul_factor in regularizators:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run();\n",
    "        print(\"Initialized for reg:\"+str(regul_factor));\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size);\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset:batch_data,tf_train_labels:batch_labels , regularization_factor : regul_factor };\n",
    "            _,l,predictions =  session.run([optimizer,loss,train_prediction],feed_dict= feed_dict);\n",
    "\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                valid_prediction.eval(), valid_labels));\n",
    "        valid_accuracy.append(accuracy(\n",
    "                valid_prediction.eval(), valid_labels));\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels));\n",
    "        test_accuracy.append(accuracy(test_prediction.eval(), test_labels));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEOCAYAAABy7Vf3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f886c1b16d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.semilogx(regularizators, valid_accuracy)\n",
    "plot.grid(True)\n",
    "plot.title('Regularization factor evaluation in CV dataset')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEOCAYAAABy7Vf3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns+kr/lNMGwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8844c11410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot.semilogx(regularizators, test_accuracy)\n",
    "plot.grid(True)\n",
    "plot.title('Regularization factor evaluation in test dataset')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization factor selected for logistic classifier: 0.00158489319246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Neural newtork(relu hidden units) with regularization\n",
    "batch_size = 128;\n",
    "graph = tf.Graph();\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size));\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels));\n",
    "    tf_validation_dataset = tf.constant(valid_dataset);\n",
    "    tf_test_set = tf.constant(test_dataset);\n",
    "    regularization_factor = tf.placeholder(tf.float32);\n",
    "    \n",
    "    #variables\n",
    "    weights_layer1 = tf.Variable(tf.truncated_normal([image_size*image_size,1024]));\n",
    "    biases_layer1 = tf.Variable(tf.zeros([1024]));\n",
    "    \n",
    "    #computaton of layer1\n",
    "    logits_layer1 = tf.matmul(tf_train_dataset,weights_layer1)+biases_layer1;\n",
    "    relus_output = tf.nn.relu(logits_layer1);\n",
    "    \n",
    "    #parametes layer2\n",
    "    weights_layer2 = tf.Variable(tf.truncated_normal([1024,10]));\n",
    "    biases_layer2 = tf.Variable(tf.zeros([10]));\n",
    "    \n",
    "    #computation layer2\n",
    "    regularization = tf.nn.l2_loss(weights_layer1) + tf.nn.l2_loss(weights_layer2);\n",
    "    logits_layer2  = tf.matmul(relus_output,weights_layer2)+biases_layer2;\n",
    "    loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits_layer2,tf_train_labels)) + (regularization_factor*regularization) ;\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss);\n",
    "    \n",
    "    #predictions\n",
    "    train_prediction = tf.nn.softmax(logits_layer2);\n",
    "    validation_relus_output = tf.nn.relu(tf.matmul(tf_validation_dataset,weights_layer1)+biases_layer1);\n",
    "    validation_prediction  = tf.nn.softmax(tf.matmul(validation_relus_output,weights_layer2)+biases_layer2);\n",
    "    test_relus_output = tf.nn.relu(tf.matmul(tf_test_set,weights_layer1)+biases_layer1);\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_relus_output,weights_layer2)+biases_layer2);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized variables for reg factor:0.0001\n",
      "Validation accuracy: 82.5%\n",
      "Test accuracy: 89.6%\n",
      "Initialized variables for reg factor:0.000125892541179\n",
      "Validation accuracy: 82.9%\n",
      "Test accuracy: 89.6%\n",
      "Initialized variables for reg factor:0.000158489319246\n",
      "Validation accuracy: 83.0%\n",
      "Test accuracy: 90.1%\n",
      "Initialized variables for reg factor:0.000199526231497\n",
      "Validation accuracy: 81.6%\n",
      "Test accuracy: 88.3%\n",
      "Initialized variables for reg factor:0.000251188643151\n",
      "Validation accuracy: 83.6%\n",
      "Test accuracy: 90.5%\n",
      "Initialized variables for reg factor:0.000316227766017\n",
      "Validation accuracy: 83.4%\n",
      "Test accuracy: 90.0%\n",
      "Initialized variables for reg factor:0.000398107170553\n",
      "Validation accuracy: 83.5%\n",
      "Test accuracy: 90.6%\n",
      "Initialized variables for reg factor:0.000501187233627\n",
      "Validation accuracy: 83.8%\n",
      "Test accuracy: 90.5%\n",
      "Initialized variables for reg factor:0.00063095734448\n",
      "Validation accuracy: 85.0%\n",
      "Test accuracy: 91.5%\n",
      "Initialized variables for reg factor:0.000794328234724\n",
      "Validation accuracy: 85.6%\n",
      "Test accuracy: 91.7%\n",
      "Initialized variables for reg factor:0.001\n",
      "Validation accuracy: 86.1%\n",
      "Test accuracy: 92.6%\n",
      "Initialized variables for reg factor:0.00125892541179\n",
      "Validation accuracy: 86.8%\n",
      "Test accuracy: 92.9%\n",
      "Initialized variables for reg factor:0.00158489319246\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 93.2%\n",
      "Initialized variables for reg factor:0.00199526231497\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 92.9%\n",
      "Initialized variables for reg factor:0.00251188643151\n",
      "Validation accuracy: 86.4%\n",
      "Test accuracy: 92.6%\n",
      "Initialized variables for reg factor:0.00316227766017\n",
      "Validation accuracy: 86.1%\n",
      "Test accuracy: 92.2%\n",
      "Initialized variables for reg factor:0.00398107170553\n",
      "Validation accuracy: 85.5%\n",
      "Test accuracy: 91.9%\n",
      "Initialized variables for reg factor:0.00501187233627\n",
      "Validation accuracy: 85.2%\n",
      "Test accuracy: 91.5%\n",
      "Initialized variables for reg factor:0.0063095734448\n",
      "Validation accuracy: 84.5%\n",
      "Test accuracy: 91.2%\n",
      "Initialized variables for reg factor:0.00794328234724\n",
      "Validation accuracy: 84.1%\n",
      "Test accuracy: 90.7%\n",
      "Initialized variables for reg factor:0.01\n",
      "Validation accuracy: 83.5%\n",
      "Test accuracy: 90.0%\n",
      "Initialized variables for reg factor:0.0125892541179\n",
      "Validation accuracy: 83.2%\n",
      "Test accuracy: 89.6%\n",
      "Initialized variables for reg factor:0.0158489319246\n",
      "Validation accuracy: 82.5%\n",
      "Test accuracy: 89.0%\n",
      "Initialized variables for reg factor:0.0199526231497\n",
      "Validation accuracy: 81.9%\n",
      "Test accuracy: 88.3%\n",
      "Initialized variables for reg factor:0.0251188643151\n",
      "Validation accuracy: 81.3%\n",
      "Test accuracy: 87.8%\n",
      "Initialized variables for reg factor:0.0316227766017\n",
      "Validation accuracy: 80.8%\n",
      "Test accuracy: 87.5%\n",
      "Initialized variables for reg factor:0.0398107170553\n",
      "Validation accuracy: 80.3%\n",
      "Test accuracy: 87.0%\n",
      "Initialized variables for reg factor:0.0501187233627\n",
      "Validation accuracy: 80.1%\n",
      "Test accuracy: 86.7%\n",
      "Initialized variables for reg factor:0.063095734448\n",
      "Validation accuracy: 79.6%\n",
      "Test accuracy: 86.3%\n",
      "Initialized variables for reg factor:0.0794328234724\n",
      "Validation accuracy: 79.2%\n",
      "Test accuracy: 85.9%\n",
      "Initialized variables for reg factor:0.1\n",
      "Validation accuracy: 78.7%\n",
      "Test accuracy: 85.4%\n",
      "Initialized variables for reg factor:0.125892541179\n",
      "Validation accuracy: 77.7%\n",
      "Test accuracy: 84.2%\n",
      "Initialized variables for reg factor:0.158489319246\n",
      "Validation accuracy: 76.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "regularizators = [pow(10,i) for i in np.arange(-4,-0.1,0.1)]\n",
    "train_accuracy = [];\n",
    "valid_accuracy = [];\n",
    "test_accuracy = [];\n",
    "\n",
    "for regul_factor in regularizators:\n",
    "    with tf.Session(graph = graph) as session:\n",
    "        tf.initialize_all_variables().run();\n",
    "        print(\"Initialized variables for reg factor:\"+str(regul_factor));\n",
    "        \n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size);\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :];\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :];\n",
    "            feed_dict = {tf_train_dataset:batch_data,tf_train_labels:batch_labels,regularization_factor:regul_factor};\n",
    "            _,l,predictions = session.run([optimizer,loss,train_prediction],feed_dict = feed_dict);\n",
    "        \n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                validation_prediction.eval(), valid_labels));\n",
    "        valid_accuracy.append(accuracy(\n",
    "                validation_prediction.eval(), valid_labels));\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels));\n",
    "        test_accuracy.append(accuracy(test_prediction.eval(), test_labels));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot.semilogx(regularizators, valid_accuracy)\n",
    "plot.grid(True)\n",
    "plot.title('Regularization factor evaluation in CV dataset')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot.semilogx(regularizators, test_accuracy)\n",
    "plot.grid(True)\n",
    "plot.title('Regularization factor evaluation in test dataset')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization factor chosed for neural network: 0.00158489319246"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables initialized\n",
      "Loss at step 0: 357.027527\n",
      "Training accuracy: 3.9%\n",
      "Validation accuracy: 31.2%\n",
      "Test accuracy: 34.4%\n",
      "Loss at step 1: 1626.959595\n",
      "Training accuracy: 30.5%\n",
      "Validation accuracy: 42.9%\n",
      "Test accuracy: 46.4%\n",
      "Loss at step 2: 1160.976074\n",
      "Training accuracy: 46.1%\n",
      "Validation accuracy: 32.5%\n",
      "Test accuracy: 34.9%\n",
      "Loss at step 3: 1062.342163\n",
      "Training accuracy: 32.8%\n",
      "Validation accuracy: 42.4%\n",
      "Test accuracy: 45.6%\n",
      "Loss at step 4: 540.718018\n",
      "Training accuracy: 55.5%\n",
      "Validation accuracy: 51.9%\n",
      "Test accuracy: 56.0%\n",
      "Loss at step 5: 272.812805\n",
      "Training accuracy: 57.0%\n",
      "Validation accuracy: 59.6%\n",
      "Test accuracy: 65.9%\n",
      "Loss at step 6: 65.631401\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 63.4%\n",
      "Test accuracy: 69.6%\n",
      "Loss at step 7: 77.229950\n",
      "Training accuracy: 75.8%\n",
      "Validation accuracy: 65.8%\n",
      "Test accuracy: 72.5%\n",
      "Loss at step 8: 53.663086\n",
      "Training accuracy: 84.4%\n",
      "Validation accuracy: 64.1%\n",
      "Test accuracy: 70.0%\n",
      "Loss at step 9: 52.185905\n",
      "Training accuracy: 85.2%\n",
      "Validation accuracy: 68.3%\n",
      "Test accuracy: 74.7%\n",
      "Loss at step 10: 31.201633\n",
      "Training accuracy: 89.1%\n",
      "Validation accuracy: 70.6%\n",
      "Test accuracy: 76.9%\n",
      "Loss at step 11: 35.927536\n",
      "Training accuracy: 90.6%\n",
      "Validation accuracy: 70.2%\n",
      "Test accuracy: 77.0%\n",
      "Loss at step 12: 6.910059\n",
      "Training accuracy: 94.5%\n",
      "Validation accuracy: 71.3%\n",
      "Test accuracy: 78.0%\n",
      "Loss at step 13: 3.548292\n",
      "Training accuracy: 93.8%\n",
      "Validation accuracy: 71.2%\n",
      "Test accuracy: 77.9%\n",
      "Loss at step 14: 10.492436\n",
      "Training accuracy: 96.1%\n",
      "Validation accuracy: 71.2%\n",
      "Test accuracy: 78.0%\n",
      "Loss at step 15: 0.416673\n",
      "Training accuracy: 98.4%\n",
      "Validation accuracy: 71.2%\n",
      "Test accuracy: 77.8%\n",
      "Loss at step 16: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 71.2%\n",
      "Test accuracy: 77.8%\n",
      "Loss at step 17: 0.164006\n",
      "Training accuracy: 99.2%\n",
      "Validation accuracy: 71.3%\n",
      "Test accuracy: 78.2%\n",
      "Loss at step 18: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 71.3%\n",
      "Test accuracy: 78.2%\n",
      "Loss at step 19: 0.000000\n",
      "Training accuracy: 100.0%\n",
      "Validation accuracy: 71.3%\n",
      "Test accuracy: 78.2%\n"
     ]
    }
   ],
   "source": [
    "#i will use 20 batches with  a smaller training set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude theres overfitting watching that training accuracy is 100% but validation is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128;\n",
    "graph = tf.Graph();\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size));\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels));\n",
    "    tf_validation_dataset = tf.constant(valid_dataset);\n",
    "    tf_test_set = tf.constant(test_dataset);\n",
    "    \n",
    "    weights_layer1=tf.Variable(tf.truncated_normal([image_size*image_size,1024]));\n",
    "    biases_layer1 = tf.Variable(tf.zeros([1024]));\n",
    "    \n",
    "    logits_layer1 = tf.matmul(tf_train_dataset,weights_layer1)+biases_layer1;\n",
    "    relus_output = tf.nn.relu(logits_layer1);\n",
    "    \n",
    "    dropout_layer = tf.nn.dropout(relus_output,0.5);\n",
    "    \n",
    "    weights_layer2 = tf.Variable(tf.truncated_normal([1024,10]));\n",
    "    biases_layer2 = tf.Variable(tf.zeros([10]));\n",
    "    \n",
    "    #regularization = tf.nn.l2_loss(weights_layer1) + tf.nn.l2_loss(weights_layer2);\n",
    "    logits_layer2 = tf.matmul(dropout_layer,weights_layer2)+biases_layer2;\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits_layer2,tf_train_labels));\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss);\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits_layer2);\n",
    "    \n",
    "    validation_relus_output = tf.nn.relu(tf.matmul(tf_validation_dataset,weights_layer1)+biases_layer1);\n",
    "    validation_prediction = tf.nn.softmax(tf.matmul(validation_relus_output,weights_layer2)+biases_layer2);\n",
    "    \n",
    "    test_relus_output = tf.nn.relu(tf.matmul(tf_test_set,weights_layer1)+biases_layer1);\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_relus_output,weights_layer2)+biases_layer2);\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables initialized\n",
      "Loss at step 0: 514.240417\n",
      "Training accuracy: 10.2%\n",
      "Validation accuracy: 24.9%\n",
      "Test accuracy: 26.0%\n",
      "Loss at step 1: 1383.257202\n",
      "Training accuracy: 18.8%\n",
      "Validation accuracy: 48.5%\n",
      "Test accuracy: 52.9%\n",
      "Loss at step 2: 1012.267578\n",
      "Training accuracy: 48.4%\n",
      "Validation accuracy: 31.4%\n",
      "Test accuracy: 34.0%\n",
      "Loss at step 3: 857.012146\n",
      "Training accuracy: 35.2%\n",
      "Validation accuracy: 52.4%\n",
      "Test accuracy: 56.9%\n",
      "Loss at step 4: 594.158569\n",
      "Training accuracy: 60.2%\n",
      "Validation accuracy: 58.8%\n",
      "Test accuracy: 64.3%\n",
      "Loss at step 5: 403.382996\n",
      "Training accuracy: 68.0%\n",
      "Validation accuracy: 62.1%\n",
      "Test accuracy: 67.5%\n",
      "Loss at step 6: 178.142731\n",
      "Training accuracy: 76.6%\n",
      "Validation accuracy: 66.1%\n",
      "Test accuracy: 71.9%\n",
      "Loss at step 7: 135.503860\n",
      "Training accuracy: 75.8%\n",
      "Validation accuracy: 69.2%\n",
      "Test accuracy: 75.1%\n",
      "Loss at step 8: 91.676674\n",
      "Training accuracy: 74.2%\n",
      "Validation accuracy: 66.0%\n",
      "Test accuracy: 72.0%\n",
      "Loss at step 9: 102.950447\n",
      "Training accuracy: 80.5%\n",
      "Validation accuracy: 70.0%\n",
      "Test accuracy: 76.6%\n",
      "Loss at step 10: 57.181427\n",
      "Training accuracy: 79.7%\n",
      "Validation accuracy: 71.6%\n",
      "Test accuracy: 77.9%\n",
      "Loss at step 11: 56.425613\n",
      "Training accuracy: 78.9%\n",
      "Validation accuracy: 70.1%\n",
      "Test accuracy: 76.4%\n",
      "Loss at step 12: 43.907665\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 73.7%\n",
      "Test accuracy: 81.1%\n",
      "Loss at step 13: 12.211496\n",
      "Training accuracy: 89.8%\n",
      "Validation accuracy: 73.5%\n",
      "Test accuracy: 81.3%\n",
      "Loss at step 14: 35.633072\n",
      "Training accuracy: 85.9%\n",
      "Validation accuracy: 70.7%\n",
      "Test accuracy: 78.0%\n",
      "Loss at step 15: 6.726592\n",
      "Training accuracy: 93.8%\n",
      "Validation accuracy: 72.0%\n",
      "Test accuracy: 78.8%\n",
      "Loss at step 16: 23.403782\n",
      "Training accuracy: 91.4%\n",
      "Validation accuracy: 73.4%\n",
      "Test accuracy: 80.9%\n",
      "Loss at step 17: 9.808304\n",
      "Training accuracy: 86.7%\n",
      "Validation accuracy: 73.0%\n",
      "Test accuracy: 79.9%\n",
      "Loss at step 18: 4.243709\n",
      "Training accuracy: 96.9%\n",
      "Validation accuracy: 73.3%\n",
      "Test accuracy: 80.0%\n",
      "Loss at step 19: 7.924232\n",
      "Training accuracy: 92.2%\n",
      "Validation accuracy: 73.5%\n",
      "Test accuracy: 80.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20;\n",
    "reduced_dataset = train_dataset[:512,:];\n",
    "reduced_labels = train_labels[:512];\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run();\n",
    "    print(\"Variables initialized\");\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (reduced_labels.shape[0] - batch_size);\n",
    "        batch_data = reduced_dataset[offset:(offset + batch_size), :];\n",
    "        batch_labels = reduced_labels[offset:(offset + batch_size), :];\n",
    "        feed_dict = {tf_train_dataset:batch_data,tf_train_labels:batch_labels};\n",
    "        _,l,predictions = session.run([optimizer,loss,train_prediction] ,feed_dict = feed_dict);\n",
    "        \n",
    "        print('Loss at step %d: %f' % (step, l))\n",
    "        print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, batch_labels))\n",
    "        print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "                validation_prediction.eval(), valid_labels));\n",
    "        print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helped but just a little bit, maybe it would give better results on bigger networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ill try with 4 layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.660305\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 17.2%\n",
      "Minibatch loss at step 500: 0.669877\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 1000: 0.569788\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1500: 0.596642\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 2000: 0.471508\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2500: 0.494823\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 3000: 0.478123\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 3500: 0.626774\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 4000: 0.442804\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 4500: 0.297537\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 5000: 0.250226\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 5500: 0.284890\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 6000: 0.474894\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 6500: 0.338899\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 7000: 0.300016\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.2%\n",
      "Minibatch loss at step 7500: 0.397618\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 8000: 0.517662\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 8500: 0.264748\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 9000: 0.301953\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 9500: 0.279435\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 10000: 0.337113\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 10500: 0.277946\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 11000: 0.237433\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 11500: 0.342979\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 12000: 0.396836\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 12500: 0.439621\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 13000: 0.291787\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 13500: 0.256885\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 14000: 0.227691\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 14500: 0.451339\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 15000: 0.467342\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 15500: 0.274702\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 16000: 0.233160\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 16500: 0.273736\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 17000: 0.336891\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 17500: 0.246500\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 18000: 0.182273\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 18500: 0.134920\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 19000: 0.206761\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 19500: 0.352944\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.0%\n",
      "  Test accuracy: 94.7%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "layer1_units = 1024;\n",
    "layer2_units = 300;\n",
    "layer3_units = 50;\n",
    "output_layer_units = num_labels;\n",
    "regularization_factor = 1e-5\n",
    "\n",
    "deep_graph = tf.Graph()\n",
    "with deep_graph.as_default():\n",
    "    \n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size));\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels));\n",
    "    tf_validation_dataset = tf.constant(valid_dataset);\n",
    "    tf_test_set = tf.constant(test_dataset);\n",
    "    \n",
    "    # hidden layer 1\n",
    "    weights_layer1=tf.Variable(tf.truncated_normal([image_size*image_size,layer1_units],stddev=np.sqrt(2.0/784)));\n",
    "    biases_layer1 = tf.Variable(tf.zeros([layer1_units]));\n",
    "    logits_layer1 = tf.matmul(tf_train_dataset,weights_layer1)+biases_layer1;\n",
    "    relus_layer1 = tf.nn.relu(logits_layer1);\n",
    "    hidden_layer1 = tf.nn.dropout(relus_layer1,0.5)\n",
    "  \n",
    "    # hidden layer 2\n",
    "    weights_layer2 = tf.Variable(tf.truncated_normal([layer1_units,layer2_units],stddev=np.sqrt(2.0/layer1_units)));\n",
    "    biases_layer2 = tf.Variable(tf.zeros([layer2_units]));\n",
    "    logits_layer2 = tf.matmul(hidden_layer1,weights_layer2)+biases_layer2;\n",
    "    relus_layer2 = tf.nn.relu(logits_layer2);\n",
    "    hidden_layer2 = tf.nn.dropout(relus_layer2,0.7);\n",
    "  \n",
    "    # hidden layer 3\n",
    "    weights_layer3 = tf.Variable(tf.truncated_normal([layer2_units,layer3_units],stddev=np.sqrt(2.0/layer2_units)));\n",
    "    biases_layer3 = tf.Variable(tf.zeros([layer3_units]));\n",
    "    logits_layer3 = tf.matmul(hidden_layer2,weights_layer3)+biases_layer3;\n",
    "    relus_layer3 = tf.nn.relu(logits_layer3);\n",
    "    hidden_layer3 = tf.nn.dropout( relus_layer3, 0.8)\n",
    "  \n",
    "    # output layer\n",
    "    weights_output_layer = tf.Variable(tf.truncated_normal([layer3_units,output_layer_units],stddev=np.sqrt(2.0/layer3_units)));\n",
    "    biases_output_layer = tf.Variable(tf.zeros([output_layer_units]));\n",
    "    logits_output_layer = tf.matmul(hidden_layer3,weights_output_layer)+biases_output_layer;\n",
    "    \n",
    "\n",
    "    # Apply regularization\n",
    "    regularization = tf.nn.l2_loss(weights_layer1)+tf.nn.l2_loss(weights_layer2)+tf.nn.l2_loss(weights_layer3)+tf.nn.l2_loss(weights_output_layer);\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits_output_layer, tf_train_labels)) + (regularization_factor*regularization)\n",
    "  \n",
    "    # Use learning rate decay on gradient descent\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 100000, 0.96, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    train_prediction = tf.nn.softmax(logits_output_layer)\n",
    "\n",
    "    # Setup validation prediction step.\n",
    "    validation_hidden_layer_1 = tf.nn.relu(tf.matmul(tf_validation_dataset, weights_layer1) + biases_layer1)\n",
    "    validation_hidden_layer_2 = tf.nn.relu(tf.matmul(validation_hidden_layer_1, weights_layer2) + biases_layer2)\n",
    "    validation_hidden_layer_3 = tf.nn.relu(tf.matmul(validation_hidden_layer_2, weights_layer3) + biases_layer3)\n",
    "    validation_logits = tf.matmul(validation_hidden_layer_3, weights_output_layer) + biases_output_layer\n",
    "    validation_prediction = tf.nn.softmax(validation_logits)\n",
    "\n",
    "    # And setup the test prediction step.  \n",
    "    test_hidden_layer_1 = tf.nn.relu(tf.matmul(tf_test_set, weights_layer1) + biases_layer1)\n",
    "    test_hidden_layer_2 = tf.nn.relu(tf.matmul(test_hidden_layer_1, weights_layer2) + biases_layer2)\n",
    "    test_hidden_layer_3 = tf.nn.relu(tf.matmul(test_hidden_layer_2, weights_layer3) + biases_layer3)\n",
    "    test_logits = tf.matmul(test_hidden_layer_3, weights_output_layer) + biases_output_layer\n",
    "    test_prediction = tf.nn.softmax(test_logits)\n",
    "\n",
    "num_steps = 20000\n",
    "\n",
    "with tf.Session(graph=deep_graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "          print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "          print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "          print(\"Validation accuracy: %.1f%%\" % accuracy(validation_prediction.eval(), valid_labels))\n",
    "    print(\"  Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
